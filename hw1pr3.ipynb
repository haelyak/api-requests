{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### week1 ~ the-web-as-filesystem...   &nbsp;&nbsp; (hw1pr3.ipynb)\n",
    "\n",
    "[the google doc with hw1's details](https://docs.google.com/document/d/11ALzpsANe3ZDR5sk8-kgaElaX_fwlDINVlQ8WS8JfQE/edit)\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Problem 3:  The \"Wisdom of the Web\":  Programming the web for inquiry!\n",
    "\n",
    "(hw1pr3.ipynb)\n",
    "\n",
    "+ Here, your task is to create a two-hop approach to \"inquire about\" a subject\n",
    "  + That is, a request, whose results are used to fashion a new request...\n",
    "  + whose results are interpreted/parsed into an answer to our original query.\n",
    "\n",
    "<br>\n",
    "\n",
    "Below is an example that uses Beautiful Soup -- that is not required.\n",
    "Feel free to use json-providing APIs (or other APIs providing structured data)\n",
    "\n",
    "<br>\n",
    "\n",
    "The assignment page has some example APIs, and there are many (many!) others!\n",
    "I encourage you to choose 1-2 of interest and explore...\n",
    "... and, from there, stitch together an \"inquiring system\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# see if you have the Beautiful Soup library...\n",
    "#\n",
    "\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\tsaik\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\tsaik\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4) (2.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 22.0.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\tsaik\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml\n",
      "  Downloading lxml-4.7.1-cp310-cp310-win_amd64.whl (3.7 MB)\n",
      "Installing collected packages: lxml\n",
      "Successfully installed lxml-4.7.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 22.0.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\tsaik\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# If you _don't_ have bs4, try installing it using pip:\n",
    "#    the pip you want is the one sharing the path of your python interpreter/kernel\n",
    "#    you can see this by clicking on the upper-left \"kernel\" pulldown\n",
    "#        for me, the kernel is /usr/local/bin/python3\n",
    "#                  so I'll use /usr/local/bin/pip3   (could be pip or pip3)\n",
    "%pip install beautifulsoup4\n",
    "\n",
    "%pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# hopefully, this now works! (if so, it will succeed silently)\n",
    "#            things to reset: (a) the python + jupyter extensions, (b) vscode itself\n",
    "\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# this should work from pr1 and pr2...\n",
    "#\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# this is the most popular HTML parser, lxml -- see if you have it (else install in the same way! :-)\n",
    "\n",
    "import lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "cs35's example :-)\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# get_school_colors_page(school)\n",
    "#\n",
    "\n",
    "\"\"\"\n",
    "Here is our strategy:\n",
    "\n",
    "1. first we grab the pages that define each school's colors (from wikipedia)\n",
    "2. then, use bs4 to parse those pages and return a list of colors (we only handle pairs of colors)\n",
    "\n",
    "3. then, grab the page that defines the popularity of colors\n",
    "4. finally, use bs4 to compute a score for each school based on its colors  (our score == sum)\n",
    "\n",
    "That's it!  \n",
    "In each case, you get to decide if it's _higher_ scores or _lower_ scores that are better :-)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup     \n",
    "\n",
    "PARSER = \"html.parser\"     # not as flexible\n",
    "PARSER = \"lxml\"            # to use this, install with .../pip install lxml\n",
    "\n",
    "#\n",
    "# this gets a school's wikipedia page and then extracts its school colors\n",
    "#\n",
    "def get_school_colors_page(school):\n",
    "    \"\"\" get_school_colors_page takes in a string with a formatted  name\n",
    "        such as Harvey_Mudd_College or Stanford_University\n",
    "        \n",
    "        it tried to request the appropriate page from wikipedia and parse\n",
    "        it with Beautiful Soup - and it should return that soup object\n",
    "    \"\"\"\n",
    "    school_color_url = \"http://en.wikipedia.org/wiki/\" + school\n",
    "    response = requests.get(school_color_url)         # request the page\n",
    "    \n",
    "    if response.status_code == 404:                 # page not found\n",
    "        print(\"For the school\", school, \"There was a problem with getting the page\")\n",
    "        print(school_color_url)\n",
    "        \n",
    "    data_from_url = response.text                   # the HTML text from the page\n",
    "    soup = BeautifulSoup(data_from_url,PARSER)      # parsed with Beautiful Soup\n",
    "    return soup\n",
    "\n",
    "\n",
    "if True:\n",
    "    #school_2 = 'Harvey_Mudd_College'\n",
    "    school_2 = 'Pitzer_College'\n",
    "    # other options that have been tested and work...\n",
    "    #school_2 = 'Pomona_College'\n",
    "    #school_2 = 'Scripps_College'\n",
    "    #school_2 = 'Claremont_McKenna_College'\n",
    "    #school_2 = 'Stanford_University'\n",
    "    # not working:\n",
    "    #school_2 = 'Harvard_University'\n",
    "    result = get_school_colors_page(school_2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tsaik\\Desktop\\CS Homework\\CS181\\week1_spr22\\week1_spr22\\hw1pr3.ipynb Cell 10'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tsaik/Desktop/CS%20Homework/CS181/week1_spr22/week1_spr22/hw1pr3.ipynb#ch0000009?line=24'>25</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m colors                     \u001b[39m# the colors... works many times, not every time.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tsaik/Desktop/CS%20Homework/CS181/week1_spr22/week1_spr22/hw1pr3.ipynb#ch0000009?line=27'>28</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tsaik/Desktop/CS%20Homework/CS181/week1_spr22/week1_spr22/hw1pr3.ipynb#ch0000009?line=28'>29</a>\u001b[0m     colors \u001b[39m=\u001b[39m extract_school_colors(result)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tsaik/Desktop/CS%20Homework/CS181/week1_spr22/week1_spr22/hw1pr3.ipynb#ch0000009?line=29'>30</a>\u001b[0m     colors\n",
      "\u001b[1;32mc:\\Users\\tsaik\\Desktop\\CS Homework\\CS181\\week1_spr22\\week1_spr22\\hw1pr3.ipynb Cell 10'\u001b[0m in \u001b[0;36mextract_school_colors\u001b[1;34m(soup)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tsaik/Desktop/CS%20Homework/CS181/week1_spr22/week1_spr22/hw1pr3.ipynb#ch0000009?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_school_colors\u001b[39m(soup):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tsaik/Desktop/CS%20Homework/CS181/week1_spr22/week1_spr22/hw1pr3.ipynb#ch0000009?line=1'>2</a>\u001b[0m     \u001b[39m\"\"\" extract_school_colors takes in a beautiful soup object, soup\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tsaik/Desktop/CS%20Homework/CS181/week1_spr22/week1_spr22/hw1pr3.ipynb#ch0000009?line=2'>3</a>\u001b[0m \u001b[39m        and uses Beautiful Soup to extract a list of all of that school's colors\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tsaik/Desktop/CS%20Homework/CS181/week1_spr22/week1_spr22/hw1pr3.ipynb#ch0000009?line=3'>4</a>\u001b[0m \u001b[39m        \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tsaik/Desktop/CS%20Homework/CS181/week1_spr22/week1_spr22/hw1pr3.ipynb#ch0000009?line=7'>8</a>\u001b[0m \u001b[39m         to obtain soup objects for each page.)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tsaik/Desktop/CS%20Homework/CS181/week1_spr22/week1_spr22/hw1pr3.ipynb#ch0000009?line=8'>9</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tsaik/Desktop/CS%20Homework/CS181/week1_spr22/week1_spr22/hw1pr3.ipynb#ch0000009?line=10'>11</a>\u001b[0m     AllDivs \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39;49mfind_all(\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m, attrs\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39mSchool colors\u001b[39m\u001b[39m\"\u001b[39m})\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tsaik/Desktop/CS%20Homework/CS181/week1_spr22/week1_spr22/hw1pr3.ipynb#ch0000009?line=11'>12</a>\u001b[0m     \u001b[39m# print(len(AllDivs))                 # debugging\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tsaik/Desktop/CS%20Homework/CS181/week1_spr22/week1_spr22/hw1pr3.ipynb#ch0000009?line=13'>14</a>\u001b[0m     \u001b[39mfor\u001b[39;00m div \u001b[39min\u001b[39;00m AllDivs:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "def extract_school_colors(soup):\n",
    "    \"\"\" extract_school_colors takes in a beautiful soup object, soup\n",
    "        and uses Beautiful Soup to extract a list of all of that school's colors\n",
    "        \n",
    "        it return that list of colors\n",
    "        \n",
    "        (Note that for different schools, you'll need to run the get_school_colors_page\n",
    "         to obtain soup objects for each page.)\n",
    "    \"\"\"\n",
    "\n",
    "    AllDivs = soup.find_all('a', attrs={\"title\":\"School colors\"})\n",
    "    # print(len(AllDivs))                 # debugging\n",
    "\n",
    "    for div in AllDivs:\n",
    "        ancestor = div.parent.parent      # this is the ancestor that holds the school colors (grandparent)\n",
    "        TDs = ancestor.findAll('td')      # it's in one of the <td> table cells </td>\n",
    "        Texts = [ x.text for x in TDs ]   # get the actual text\n",
    "\n",
    "        if len(Texts) < 1:                # didn't get any? a problem!\n",
    "            print(f\"No colors found!\")\n",
    "            return []\n",
    "\n",
    "        s = Texts[0]                      # take the first one...\n",
    "        colors = get_text_with_and(s)     # use our helper function\n",
    "        return colors                     # the colors... works many times, not every time.\n",
    "\n",
    "\n",
    "if True:\n",
    "    colors = extract_school_colors(result)\n",
    "    colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#$ this & that !?\n",
      "result is ['this', 'that']\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# a helper function to handle the messy alternatives, e.g., Black & gold, Blue and white\n",
    "# \n",
    "\n",
    "import string \n",
    "\n",
    "DEBUG = True\n",
    "\n",
    "def get_text_with_and(s):\n",
    "    \"\"\" first, finds 'and' or '&' in s \n",
    "        then finds all of the alphabetic \n",
    "        characters in either direction from that conjunction!\n",
    "    \"\"\"\n",
    "    allowable = ' '+string.ascii_lowercase  # allowable letters\n",
    "    s = s.lower()                           # only consider lower case\n",
    "    if DEBUG: print(s)                                # for debugging\n",
    "    if ('and' in s or '&' in s) == False:   # is there an 'and' or '&'?  If not...\n",
    "        print(\"No 'and' found...\")          #    return the lowercased s\n",
    "        return s\n",
    "\n",
    "    and_index = s.find('and')               # Try 'and' first\n",
    "    if and_index == -1:                     # If it's not there,\n",
    "        and_index = s.find('&')             # Try '&'\n",
    "        if and_index == -1:                 # This should have been caught, but I'm superstitious\n",
    "            print(\"No 'and' found...\")\n",
    "            return s\n",
    "        left = s[:and_index]                # get the left and right pieces\n",
    "        right = s[and_index+1:]             # '&' is one char long, hence the 1 \n",
    "    else:\n",
    "        left = s[:and_index]                # get the left and right pieces\n",
    "        right = s[and_index+3:]             # 'and' is three chars long, hence the 3\n",
    "\n",
    "    # print(f\"left, right are {left,right}\") # for debugging -- now, get the color on the left and right:\n",
    "\n",
    "    new_left = ''\n",
    "    for c in left[::-1]:   # go backwards\n",
    "        if c not in allowable:\n",
    "            break\n",
    "        else:\n",
    "            new_left = c + new_left          # accumulate, as long as it's a character\n",
    "    left = new_left\n",
    "\n",
    "    new_right = ''\n",
    "    for c in right[:]:   # go forwards\n",
    "        if c not in allowable:\n",
    "            break\n",
    "        else:\n",
    "            new_right = new_right + c        # accumulate, as long as it's a character\n",
    "    right = new_right\n",
    "\n",
    "    left = left.strip()                      # remove whitespace on either side\n",
    "    right = right.strip()                    # remove whitespace on either side\n",
    "    #print(f\"left, right are now {left,right}\")  # more debugging!\n",
    "\n",
    "    return [left, right]                     # only two colors for this scraper!\n",
    "\n",
    "if True:\n",
    "    result = get_text_with_and(\"#$ this & that !?\")\n",
    "    print(f\"result is {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score for blue is 1\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Color-page handling\n",
    "# \n",
    "\n",
    "# this line imports the HTML parser named BeautifulSoup:\n",
    "#      it's a class constructor; when called it will return an object of type BeautifulSoup\n",
    "from bs4 import BeautifulSoup     \n",
    "\n",
    "PARSER = \"html.parser\"     # not as flexible a parser\n",
    "PARSER = \"lxml\"            # to use lxml (the most common), you'll need to install with .../pip install lxml\n",
    "\n",
    "#\n",
    "# get_color_page()\n",
    "#\n",
    "def get_color_page():\n",
    "    \"\"\" This function requests the most-popular-colors page\n",
    "        and parses it with Beautiful Soup, returning the resulting\n",
    "        Beautiful Soup object, soup\n",
    "    \"\"\"\n",
    "    color_popularity_url = \"http://www.thetoptens.com/top-ten-favorite-colors/\"\n",
    "    response = requests.get(color_popularity_url)   # request the page\n",
    "\n",
    "    if response.status_code == 404:                 # page not found\n",
    "        print(\"There was a problem with getting the page:\")\n",
    "        print(color_popularity_url)\n",
    "\n",
    "    data_from_url = response.text                   # the HTML text from the page\n",
    "    #soup = BeautifulSoup(data_from_url,\"lxml\")      # parsed with Beautiful Soup\n",
    "    soup = BeautifulSoup(data_from_url,PARSER)      # parsed with Beautiful Soup\n",
    "    return soup\n",
    "\n",
    "\n",
    "#\n",
    "# find_color_score( color, soup )\n",
    "#\n",
    "def find_color_score( color_name, soup ):\n",
    "    \"\"\" find_color_score takes in color_name (a string represnting a color)\n",
    "        and soup, a Beautiful Soup object returned from a successful run of\n",
    "        get_color_page\n",
    "        \n",
    "        find_color_score returns our predictive model's number of points in\n",
    "        a potential match up involving a team with that color\n",
    "        \n",
    "        the number of points is 21 - ranking, where ranking is from 1 (most\n",
    "        popular color) to 20 (least popular color) or 21, representing all\n",
    "        of the others\n",
    "    \"\"\"\n",
    "    ListOfDivs = soup.find_all('div', {'class':\"i\"})   # the class name happens to be 'i' here...\n",
    "    ranking = 1\n",
    "    for div in ListOfDivs:\n",
    "        # print(div.em, div.b)                    # checking the subtags named em and b\n",
    "        this_divs_color = div.b.text.lower()      # getting the text from them (lowercase)\n",
    "        this_divs_ranking = 21                    # the deafult (integer) ranking: 21\n",
    "        try:\n",
    "            this_divs_ranking = int(ranking)      # try to convert it to an integer\n",
    "        except:                                   # if it fails\n",
    "            pass                                  # do nothing and leave it at 21\n",
    "        \n",
    "        if color_name == this_divs_color:         # check if we need to return this one\n",
    "            return this_divs_ranking\n",
    "        ranking +=1\n",
    "    # if we ran through the whole for loop without finding a match, the ranking is 21\n",
    "    return 21\n",
    "\n",
    "\n",
    "if True:\n",
    "    color_soup = get_color_page()      # should run this once!\n",
    "    color_to_test = \"blue\"\n",
    "    score = find_color_score(color_to_test,color_soup)\n",
    "    print(f\"score for {color_to_test} is {score}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done scraping the school colors.\n",
      "\n",
      "green and white  [5]\n",
      "blue and white[1][a]   \n",
      "\n",
      "School 1 (Scripps_College) colors: ['green', 'white']\n",
      "School 2 (Pomona_College) colors: ['blue', 'white']\n",
      "\n",
      "Done scraping the color-popularity page.\n",
      "\n",
      "School 1 (Scripps_College) scores: [3, 9]\n",
      "School 2 (Pomona_College) scores: [1, 9]\n",
      "\n",
      "School 1 (Scripps_College) total score: 12\n",
      "School 2 (Pomona_College) total score: 10\n",
      "Scripps_College  wins!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#\n",
    "# put it all together!\n",
    "#\n",
    "if True:\n",
    "    \"\"\"\n",
    "    # Here is an example of using the Web's Wisdom to answer the question\n",
    "    #\n",
    "    #     Which college is best.....?\n",
    "    #\n",
    "    #     Not limited to the 5Cs!\n",
    "    \"\"\"\n",
    "    #school_1 = 'Harvey_Mudd_College'\n",
    "    #school_1 = 'Pitzer_College'\n",
    "    school_1 = 'Scripps_College'\n",
    "    #other options (that have been tested and work)...\n",
    "    school_2 = 'Pomona_College'\n",
    "    #school_2 = 'Claremont_McKenna_College'\n",
    "    #school_2 = 'Stanford_University'\n",
    "    #school_2 = 'Harvard_University'\n",
    "    # \n",
    "    # As a reminder, here is our \"web scavenging\" approach:\n",
    "    #\n",
    "    # 1. first we grab the pages that define each school's colors (wikipedia)\n",
    "    # 2. then, use bs4 to parse those pages and return a list of colors\n",
    "    # 3. then, grab the page that defines the popularity of colors\n",
    "    # 4. finally, use bs4 to compute a score for each school based on its colors\n",
    "    #\n",
    "\n",
    "    # we get the school colors page for each school\n",
    "    # and we return a BeautifulSoup \"soup\" object for each!\n",
    "    school_soup_1 = get_school_colors_page(school_1)\n",
    "    school_soup_2 = get_school_colors_page(school_2)\n",
    "    print(\"Done scraping the school colors.\\n\")\n",
    "\n",
    "    # We have a function that actually grabs the colors from the page...\n",
    "    school_colors_1 = extract_school_colors( school_soup_1 )\n",
    "    school_colors_2 = extract_school_colors( school_soup_2 )\n",
    "    print()\n",
    "    print(\"School 1 (\" + school_1 + \") colors:\", school_colors_1)\n",
    "    print(\"School 2 (\" + school_2 + \") colors:\", school_colors_2)\n",
    "\n",
    "\n",
    "    # Next, we grab the color-popularity page (and parse it into\n",
    "    # a BeautifulSoup object...\n",
    "    # \n",
    "    color_popularity_soup = get_color_page()\n",
    "    print(\"\\nDone scraping the color-popularity page.\\n\")\n",
    "\n",
    "    # Finally, we convert the team colors into total scores\n",
    "    # which will reveal our predicted result\n",
    "    # Admittedly, our \"points\" are simply the ranking of how popular a color is.\n",
    "\n",
    "    # let's use a list comprehension as a reminder of how those work...\n",
    "    school_1_scores = [ find_color_score(clr, color_popularity_soup) for clr in school_colors_1 ]\n",
    "    school_2_scores = [ find_color_score(clr, color_popularity_soup) for clr in school_colors_2 ]\n",
    "    print(\"School 1 (\" + school_1 + \") scores:\", school_1_scores)\n",
    "    print(\"School 2 (\" + school_2 + \") scores:\", school_2_scores)\n",
    "    print()\n",
    "    print(\"School 1 (\" + school_1 + \") total score:\", sum(school_1_scores))\n",
    "    print(\"School 2 (\" + school_2 + \") total score:\", sum(school_2_scores))\n",
    "\n",
    "    #find the winner\n",
    "    if sum(school_1_scores) > sum(school_2_scores):\n",
    "        print(school_1, \" wins!\")\n",
    "    elif sum(school_1_scores) < sum(school_2_scores):\n",
    "        print(school_2, \" wins!\")\n",
    "    else:\n",
    "        print(\"It's a tie!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# your scavenger hunt - or question-answering - need not \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Your task!     \n",
    "#\n",
    "\n",
    "#\n",
    "# Beautiful soup is not required!  Feel free to try it, if you'd like...\n",
    "# \n",
    "# Your task is to choose 1-2 new web APIs (or web pages) and then\n",
    "# create a two-step process that answers a question \n",
    "#      (as you see, both serious and not-so-serious questions are welcome!)\n",
    "# \n",
    "# That is, there should be a web-access (API or beautiful-soup-scraping) to obtain a first piece of information\n",
    "# then, use that to create a second web-access (same API, different API, or another raw page)\n",
    "# from which you construct your answer!\n",
    "#\n",
    "# As you see in this example, it does not _always_ need to work (Aargh! Harvard! ;-)\n",
    "#\n",
    "# But, it should work for several examples -- show off with a few of them :-)\n",
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'results': [{'id': 655055, 'title': 'Pea Soup With Smoked Pork Ribs', 'image': 'https://spoonacular.com/recipeImages/655055-312x231.jpg', 'imageType': 'jpg'}, {'id': 652078, 'title': 'Miso Soup With Thin Noodles', 'image': 'https://spoonacular.com/recipeImages/652078-312x231.jpg', 'imageType': 'jpg'}, {'id': 633194, 'title': 'Azteca Soup', 'image': 'https://spoonacular.com/recipeImages/633194-312x231.jpg', 'imageType': 'jpg'}, {'id': 647631, 'title': 'Hummus Soup', 'image': 'https://spoonacular.com/recipeImages/647631-312x231.jpg', 'imageType': 'jpg'}, {'id': 649933, 'title': 'Lentil Soup with Chorizo', 'image': 'https://spoonacular.com/recipeImages/649933-312x231.jpg', 'imageType': 'jpg'}], 'offset': 0, 'number': 5, 'totalResults': 286}\n",
      "{'results': [{'id': 661351, 'title': 'Spinach Soup With Wontons', 'image': 'https://spoonacular.com/recipeImages/661351-312x231.jpg', 'imageType': 'jpg'}, {'id': 643011, 'title': 'Five Spice Chinese Pork Stew', 'image': 'https://spoonacular.com/recipeImages/643011-312x231.jpg', 'imageType': 'jpg'}], 'offset': 0, 'number': 5, 'totalResults': 2}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# web-access 1\n",
    "#\n",
    "\n",
    "def get_recipes(food, cuisine):\n",
    "    \"\"\" get_recipes takes in two strings\n",
    "        food: the type of dish we want to look at\n",
    "        cuisine: the cuisine of the food we are looking for\n",
    "        \n",
    "        it will return the json of all the recipes of that type of food and that cuisine\n",
    "    \"\"\"\n",
    "    search_url = \"https://api.spoonacular.com/recipes/complexSearch\" + \"?apiKey=b3611b53d22d4fc3b6bc7ac565fe4acf\"\n",
    "\n",
    "\n",
    "\n",
    "    parameters = { \"query\": food,\n",
    "                    \"cuisine\": cuisine,\n",
    "                    \"number\": 5\n",
    "                }\n",
    "\n",
    "    headers = {\n",
    "            \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    response = requests.get(search_url, params=parameters, headers=headers)         # request the page\n",
    "    data = response.json()\n",
    "    #print(data)\n",
    "    \n",
    "    if response.status_code == 404:                 # page not found\n",
    "        print(\"For the cuisine\", cuisine, \"There was a problem with getting the page\")\n",
    "        print(search_url)\n",
    "    return data\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "if True:\n",
    "    cuisine = \"british\"\n",
    "    cuisine2 = \"chinese\"\n",
    "    food = \"soup\"\n",
    "    result1 = get_recipes(food,cuisine) \n",
    "    result2 = get_recipes(food, cuisine2)\n",
    "    print(result1)\n",
    "    print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'calories': '376k', 'carbs': '28g', 'fat': '18g', 'protein': '22g', 'bad': [{'title': 'Calories', 'amount': '376k', 'indented': False, 'percentOfDailyNeeds': 18.82}, {'title': 'Fat', 'amount': '18g', 'indented': False, 'percentOfDailyNeeds': 29.16}, {'title': 'Saturated Fat', 'amount': '11g', 'indented': True, 'percentOfDailyNeeds': 70.96}, {'title': 'Carbohydrates', 'amount': '28g', 'indented': False, 'percentOfDailyNeeds': 9.65}, {'title': 'Sugar', 'amount': '5g', 'indented': True, 'percentOfDailyNeeds': 5.89}, {'title': 'Cholesterol', 'amount': '66mg', 'indented': False, 'percentOfDailyNeeds': 22.12}, {'title': 'Sodium', 'amount': '894mg', 'indented': False, 'percentOfDailyNeeds': 38.88}], 'good': [{'title': 'Protein', 'amount': '22g', 'indented': False, 'percentOfDailyNeeds': 45.68}, {'title': 'Selenium', 'amount': '34µg', 'indented': False, 'percentOfDailyNeeds': 49.81}, {'title': 'Calcium', 'amount': '421mg', 'indented': False, 'percentOfDailyNeeds': 42.14}, {'title': 'Phosphorus', 'amount': '359mg', 'indented': False, 'percentOfDailyNeeds': 35.91}, {'title': 'Vitamin B12', 'amount': '1µg', 'indented': False, 'percentOfDailyNeeds': 20.42}, {'title': 'Vitamin A', 'amount': '982IU', 'indented': False, 'percentOfDailyNeeds': 19.66}, {'title': 'Vitamin B2', 'amount': '0.33mg', 'indented': False, 'percentOfDailyNeeds': 19.28}, {'title': 'Manganese', 'amount': '0.38mg', 'indented': False, 'percentOfDailyNeeds': 18.78}, {'title': 'Zinc', 'amount': '2mg', 'indented': False, 'percentOfDailyNeeds': 17.6}, {'title': 'Potassium', 'amount': '464mg', 'indented': False, 'percentOfDailyNeeds': 13.27}, {'title': 'Magnesium', 'amount': '47mg', 'indented': False, 'percentOfDailyNeeds': 11.78}, {'title': 'Copper', 'amount': '0.21mg', 'indented': False, 'percentOfDailyNeeds': 10.4}, {'title': 'Vitamin E', 'amount': '1mg', 'indented': False, 'percentOfDailyNeeds': 9.94}, {'title': 'Iron', 'amount': '1mg', 'indented': False, 'percentOfDailyNeeds': 9.76}, {'title': 'Fiber', 'amount': '2g', 'indented': False, 'percentOfDailyNeeds': 9.14}, {'title': 'Vitamin B6', 'amount': '0.18mg', 'indented': False, 'percentOfDailyNeeds': 8.75}, {'title': 'Vitamin C', 'amount': '6mg', 'indented': False, 'percentOfDailyNeeds': 7.8}, {'title': 'Vitamin B3', 'amount': '1mg', 'indented': False, 'percentOfDailyNeeds': 7.49}, {'title': 'Folate', 'amount': '25µg', 'indented': False, 'percentOfDailyNeeds': 6.35}, {'title': 'Vitamin B5', 'amount': '0.61mg', 'indented': False, 'percentOfDailyNeeds': 6.14}, {'title': 'Vitamin B1', 'amount': '0.07mg', 'indented': False, 'percentOfDailyNeeds': 4.66}, {'title': 'Vitamin K', 'amount': '4µg', 'indented': False, 'percentOfDailyNeeds': 4.09}, {'title': 'Vitamin D', 'amount': '0.31µg', 'indented': False, 'percentOfDailyNeeds': 2.08}], 'expires': 1639102983355, 'isStale': True}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# web-access 2\n",
    "#\n",
    "\n",
    "def get_nutrition(id):\n",
    "    \"\"\" get_nutrition takes in one string\n",
    "        id: the id of a specific recipe\n",
    "        \n",
    "        it will return the json of all the nutrition information for a given recipe\n",
    "    \"\"\"\n",
    "    search_url = \"https://api.spoonacular.com/recipes/\" + id + \"/nutritionWidget.json?apiKey=b3611b53d22d4fc3b6bc7ac565fe4acf\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    response = requests.get(search_url, headers=headers)         # request the page\n",
    "    data = response.json()\n",
    "    # print(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "if True:\n",
    "    results = get_nutrition(\"633876\")\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are going to look at whether italian noodles is healthier than japanese noodles.\n",
      "\n",
      "Got the recipes of italian noodles.\n",
      "Got the recipes of japanese noodles.\n",
      "\n",
      "Got the total grams of fat for all italian noodles recipes.\n",
      "Got the total grams of fat for all japanese noodles recipes.\n",
      "\n",
      "italian has a total of 90 g of fat and japanese has a total of 42 g of fat.\n",
      "So japanese is healthier!\n",
      "\n",
      "We are going to look at whether italian pasta is healthier than american pasta.\n",
      "\n",
      "Got the recipes of italian pasta.\n",
      "Got the recipes of american pasta.\n",
      "\n",
      "Got the total grams of fat for all italian pasta recipes.\n",
      "Got the total grams of fat for all american pasta recipes.\n",
      "\n",
      "italian has a total of 58 g of fat and american has a total of 58 g of fat.\n",
      "So they are both equally healthy!\n",
      "\n",
      "We are going to look at whether chinese rice is healthier than korean rice.\n",
      "\n",
      "Got the recipes of chinese rice.\n",
      "Got the recipes of korean rice.\n",
      "\n",
      "Got the total grams of fat for all chinese rice recipes.\n",
      "Got the total grams of fat for all korean rice recipes.\n",
      "\n",
      "chinese has a total of 54 g of fat and korean has a total of 78 g of fat.\n",
      "So chinese is healthier!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# putting it all together!\n",
    "#\n",
    "\n",
    "def healthier_cuisine(food, cuisine1, cuisine2 ):\n",
    "    \"\"\" healtheir_cuisine takes in three strings\n",
    "        food: the type of food we are analyzing\n",
    "        cuisine1: the first cuisine we are looking at\n",
    "        cuisine2: the second cuisine we are looking at\n",
    "\n",
    "        returns whichever recipe is more healthy based on fat levels\n",
    "        \n",
    "    \"\"\"\n",
    "    recipes1 = get_recipes(food, cuisine1)\n",
    "    print(f\"Got the recipes of {cuisine1} {food}.\")\n",
    "    recipes2 = get_recipes(food, cuisine2)\n",
    "    print(f\"Got the recipes of {cuisine2} {food}.\")\n",
    "    print()\n",
    "\n",
    "    sumFat1 = 0\n",
    "    sumFat2 = 0\n",
    "\n",
    "    i = 0\n",
    "    listRecipes1 = recipes1[\"results\"]\n",
    "    while i < len(listRecipes1):\n",
    "        id = str(recipes1[\"results\"][i][\"id\"])\n",
    "        nutrition1 = get_nutrition(id)\n",
    "        sumFat1 += int(nutrition1[\"fat\"][:-1])\n",
    "        i+=1\n",
    "    print(f\"Got the total grams of fat for all {cuisine1} {food} recipes.\")    \n",
    "    \n",
    "    j = 0\n",
    "    while j < len(recipes2[\"results\"]):\n",
    "        id = str(recipes2[\"results\"][j][\"id\"])\n",
    "        nutrition2 = get_nutrition(id)\n",
    "        sumFat2 += int(nutrition2[\"fat\"][:-1])\n",
    "        j+=1\n",
    "    print(f\"Got the total grams of fat for all {cuisine2} {food} recipes.\")  \n",
    "    print()\n",
    "    \n",
    "\n",
    "    if  sumFat1 > sumFat2:\n",
    "        print(f\"{cuisine1} has a total of {sumFat1} g of fat and {cuisine2} has a total of {sumFat2} g of fat.\")\n",
    "        print(f\"So {cuisine2} is healthier!\")\n",
    "    elif  sumFat1 < sumFat2:\n",
    "        print(f\"{cuisine1} has a total of {sumFat1} g of fat and {cuisine2} has a total of {sumFat2} g of fat.\")\n",
    "        print(f\"So {cuisine1} is healthier!\")\n",
    "    else:\n",
    "        print(f\"{cuisine1} has a total of {sumFat1} g of fat and {cuisine2} has a total of {sumFat2} g of fat.\")\n",
    "        print(f\"So they are both equally healthy!\")\n",
    "\n",
    "if True:\n",
    "    cuisine = \"italian\"\n",
    "    cuisine2 = \"japanese\"\n",
    "    food = \"noodles\"\n",
    "    print(f\"We are going to look at whether {cuisine} {food} is healthier than {cuisine2} {food}.\")\n",
    "    print()\n",
    "    result = healthier_cuisine(food, cuisine, cuisine2)\n",
    "    print()\n",
    "\n",
    "    cuisine = \"italian\"\n",
    "    cuisine2 = \"american\"\n",
    "    food = \"pasta\"\n",
    "    print(f\"We are going to look at whether {cuisine} {food} is healthier than {cuisine2} {food}.\")\n",
    "    print()\n",
    "    result = healthier_cuisine(food, cuisine, cuisine2)\n",
    "    print()\n",
    "\n",
    "    cuisine = \"chinese\"\n",
    "    cuisine2 = \"korean\"\n",
    "    food = \"rice\"\n",
    "    print(f\"We are going to look at whether {cuisine} {food} is healthier than {cuisine2} {food}.\")\n",
    "    print()\n",
    "    result = healthier_cuisine(food, cuisine, cuisine2)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ccb4bb6bd67730c9185e6c24c983362cd7b4575b595bfae100d8d91e48f4f1e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
